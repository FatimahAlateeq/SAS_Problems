### Standard operating procedure (SOP): BioEquivalence Statistical Analytics of 2x2 cross over study using SAS Software

____

### Before start: This Note's info

**Department:** Statistics.

**Project:** Bioequivalence. 

**Last Update:** 30/Oct/2024

**Version:** 2.0

**Is it an official copy?** No, This is a <u>draft copy</u>.

**Edited by:** Fatimah in 30/Oct/2024.

___

**Editor notes 23/Oct/2024:**

1. This copy of SOP'S uses a personal account of <u>[SAS OnDemand For Academics](https://welcome.oda.sas.com/)</u>. This account is a 1 year free trail, now it is 6 months remaining. (version info: 
   
   ```
     Release: 3.81 (Enterprise Edition)
     Build date: 27 Apr 2022 13:21:36  
     SAS release: 9.04.01M7P08062020  
     SAS platform: Linux LIN X64 5.14.0-284.30.1.el9_2.x86_64  
     Site name: SAS ONDEMAND FOR ACADEMICS  
     Site number:70094220
   ```
   
   )

2. This copy of SOP'S uses a work account of MicroSoft Excel to manage <u>.xlsx</u> files. (version info: 
   
   ```
   Microsoft® Excel® for Microsoft 365 MSO (Version 2305 Build 16.0.16501.20074) 64-bit 
   ```
   
   )

3. All shown sensitive data in this note is fake (concentrations also faked), but have same appearance.

4. This note suitable for 2 sequences x 2 period cross over study.

5. These procedures need prior knowledge and  basic skills about programming debugging (at least Python *or* SAS *or* R...etc).

6. You need internet.

7. I advice you to have some basic information of SAS interface. (less than 1 hour to learn about it, its free in internet).

8. Each code is related to a previous procedure's code.

9. I am not sure about my calculations (I did not validate them yet).

____

##### Definitions:

**PI**: Principal Investigator.

___

### Procedures

#### 0. Prepare folder of raw files

**0.1** Make copy of any file you receive and store the original and the copy in folder of raw files for the specific study with version number. (example folder name: /raw files of Panadol-Paracetamol BioEq Study/).

**0.2** Organize your files with meaningful names and versions with dates of edits 

**0.3** Write <u>.txt</u> file that describe the folder contents and ideas and any important info in detail. (example folder name: /raw files of Panadol-Paracetamol BioEq Study/info.txt).

____

#### 1. Have SAS Software and .xlsx manager software

**1.1** Get your own work <u>username</u> and <u>password</u> from licensed account of SAS software by your company.

**1.2** Prepare a tool to manage <u>.xlsx</u> files (examples: *Microsoft Excel*, *Google Sheets*. **DO NOT** use personal accounts! only your work licenced accounts).

____

#### 2. Data pre-processing (preparing data using Excel)

**2.1** You supposed to receive the data in this form or similar in <u>.xlsx</u> file type: 

![figure: raw data](Images/2024-10-21-11-26-52-image.png)

**2.2** `Time-Point` columns **most NOT** contain empty value, symbols, strings, nor Characters, **Only** numbers. (example: see *yellow* cells in [figure: raw data]). If it contains unacceptable values contact the *PI*. **DO NOT** Edit them without a permission via formal Email from the *PI* (WhatsApp, Call, Speech, Face to face,...etc. Are **NOT** acceptable, **ONLY** Email).

**2.3** Rearrange the data in a proper form as structured table, without descriptive statistics. (example: see *orange* cells in [figure: raw data]).

**2.4** Raw data most contains these info: `subject number`, `sequence`, `treatment`(test drug, reference drug), `time-points`. If the raw data missing any of them, you **CAN NOT** do analysis, ask *PI* to receive the data again with complete info.

**2.5** Clean data supposed to be like this table: (`period` could be generated by you, if did not received in the raw data)

![figure: clean data](Images/2024-10-21-12-04-44-image.png)

**2.6** Generate new column named `period` from `sequence` and `treatment` using a correct formula. (example: ![](Images/2024-10-21-12-13-35-image.png)). This column describes the period of the drug have been taken in.

____

#### 3. Importing cleaned data to SAS and environment preparing

**3.1** Import cleaned data type <u>.xlsx</u> file into SAS environment. 

1. Get into your SAS account <u><a href="https://welcome.oda.sas.com/">SAS OnDemand For Academics</a></u> >> select folder `sasuser.v94` >> click import icon>> [see figure: SAS import .xlsx file]
       ![figure: SAS import .xlsx file 3.1.1](Images/2024-10-22-09-58-27-image.png)
   ![figure: SAS import .xlsx file 3.1.2](Images/2024-10-22-10-15-06-image.png)

2. Drop down the folder `sasuser.v94`, find our <u>.xlsx</u> file then copy its path from its properties.
   ![figure: SAS import .xlsx file 3.2.1](Images/2024-10-22-10-30-44-image.png)

3. Open new SAS page and save it to name it (ctrl+s).
   ![figure: SAS open new SAS-page 3.3.1](Images/2024-10-22-10-35-51-image.png)![](Images/2024-10-22-10-45-15-image.png)

4. Now you can write SAS codes. follow this code and read my comments that explain every line

5. To run any line or any couple of lines, or all page: select the lines>>click the <u>runner-man icon</u>.
   
   ```sas
   /* import .xlsx file into this workspace */
   libname lib03 XLSX "/home/u6******0/sasuser.v94/Table_of_Concentration.xlsx"; * paste the copied path;
   
   /* copy the data into 'work' library, by importing the worksheet name, mine is:TOC_clean */
   data work.my_data;
       set lib03.'TOC_clean'n;
       * you can filter subjects in this step using this line for Example: 
        Where 'subject number'n not eq 4;
   run;
   ```

6. **What if you faced Errors?** read them and figure out your mistakes or copy the error and paste it in browser search and find the solution (use your debugging skills).

7. **If a `subject` is missing in any period**, **Do Not** include hem in the analysis, and inform the PI about that and note it in your report.

____

#### 4. Prepare variables

**4.1** Do macros to make it easy when you call variables in the SAS-Procedures(proc).

**4.2** Transform `Time-points` columns to 2-columns named `Time_point` and `concentration_value`, that to apply the procs easily with correct analysis. 

```sas
/* make macro for each neaded variable */
%let my_data=my_data;
%let subj='subject number'n;
%let sequ=sequence;
%let trt=treatment;
%let prd=period;
%let parameters='0'n '0.08'n '0.25'n '0.5'n '0.75'n '1'n '1.33'n '1.67'n '2'n '2.33'n '2.67'n '3'n '3.33'n '3.67'n '4'n '4.5'n '5'n '6'n '8'n '10'n '12'n '16'n '24'n '36'n '48'n;
* these are the time points from the data;

/* transform time-points */
%let length_parameters=%sysfunc(countw(&parameters, %str('n '))); * '%sysfunc(countw()' conceders the '.' as separator so I add '%str('n ')' argument. this macro is to get count of the parameters;

data transformed;
    set &my_data;
    array parameters &parameters; * Create an array for the parameters;
    array parameters_names{&length_parameters} $ &parameters; * Corresponding names for the array;
    do i=1 to &length_parameters; * Loop through each parameter;
        time_point=input(strip(vname(parameters{i})), 8.); * Get the name of the time point;
        concentration_value=input(strip(parameters{i}), 8.); * Get the value of concentration at that time point;
        output; * Output the transformed row;
    end;
    drop i &parameters; * Drop the loop variable 'i' and transformed parameters;
run;

proc sort data=transformed; * sort the data, it is needed for next steps;
    by &subj &sequ &trt &prd; * I call a macro after '&' sign;
run; 
```

____

#### 5. Calculations

**5.1** You need some calculations like C_max, AUC, T_max, and log...etc.

```sas
/* calculations */
*calculate AUC using Trapezoid technique;
DATA auc_temp;
    SET transformed;
    by &subj &sequ &trt &prd;
    LagTime=LAG(time_point);
    LagValue=LAG(concentration_value);

    IF time_point=0 THEN
        DO;
            LagTime=0;
            LagValue=0;
        END;
    Trapezoid=(time_point-LagTime)*(concentration_value+LagValue)/2;
RUN;

* calculate geomean with limits, and cv;
proc surveymeans data=transformed geomean UGMCLM LGMCLM CV alpha=.1;
    /* class 'subject number'n sequence type prd; */
    by &subj &sequ &trt &prd;
    var concentration_value;
    where not concentration_value eq 0;
    * some calculations does not accept 0 value;
    ods output summary=summary;
    ods output statistics=statistics;
    ods output summaryPanel=summarypanel;
    ods output geometricmeans=geometricmeans;
run;

data geo_temp;
    merge summary (where=(Label1='Number of Observations')) statistics 
        geometricmeans;
    rename geomean=geomean u1sideclgm=upper_gmc_lm_90 l1sideclgm=lower_gmc_lm_90 
        cv=cv;
    drop label1 cvalue1 nValue1 varname;
run;

/* *calculat c_max, total AUC, t_max, and logs; */
proc sql;
    create table cmax_auc_temp as select &subj, &sequ, &trt, &prd, 
        avg(concentration_value) as mean, std(concentration_value) as stdv, 
        max(concentration_value) as cmax , sum(Trapezoid) as auc_0t , 
        log(max(concentration_value)) as log_Cmax, log(sum(Trapezoid)) as log_auc_0t 
        from auc_temp group by &subj, &sequ, &trt, &prd;
quit;

proc sql;
    create table calcs_summary_1 as select cmax_auc_temp.*, transformed.time_point as 
        t_max, geo_temp.geomean, geo_temp.gmstderr, geo_temp.cv, 
        geo_temp.lower_gmc_lm_90, geo_temp.upper_gmc_lm_90 from cmax_auc_temp left 
        join transformed on cmax_auc_temp.cmax=transformed.concentration_value left 
        join geo_temp on cmax_auc_temp.&subj=geo_temp.&subj and 
        cmax_auc_temp.&sequ=geo_temp.&sequ and cmax_auc_temp.&trt=geo_temp.&trt and 
        cmax_auc_temp.&prd=geo_temp.&prd
order by &subj, &sequ, &trt, &prd;
quit;

title color=red "Calculations Table summary";

proc print data=calcs_summary_1;
run;

title;
```

____

#### 6. Outliers detection

**6.0** Outliers are values at the extreme ends of a dataset. Some outliers represent true values from natural variation in the population. Other outliers may result from incorrect data entry, equipment malfunctions, or other measurement errors. 

**6.0.1** When you find outliers, report the *PI* about the outlier observation, with the used statistical test's result that appears it.

**6.1** You need to encode categorical variables to apply models.

```sas
*** make macros ***;
%let data_set=calcs_summary_1;
%let id=&subj;
%let categorical_variables=&sequ &trt; * select catigorical variables to encode them next step;
%let independent_variables=e_&sequ e_&trt &prd; * 'e_' means encoded;
%let dependent_variables=Cmax auc_0t;

* encoding categorical variables, it is needed to apply models;
%macro label_encode(dataset,var);
   proc sql noprint;
     select distinct(&var)
     into:val1-
     from &dataset;
 select count(distinct(&var))  into:mx from &dataset;
 quit;
 data &dataset;
     set &dataset;
   %do i=1 %to &mx;
     if (compress(strip(&var), '() /') = compress("&&&val&i", '() /')) then e_&var=&i;
   %end;
   run;
 %mend;
%macro encode_list_variables(dataset, var_list);
   %let var_count = %sysfunc(countw(&var_list));
   %do j = 1 %to &var_count;
     %let var = %scan(&var_list, &j);
     %label_encode(&dataset, &var);
   %end;
%mend;

*use the function;
%encode_list_variables(&data_set, &categorical_variables);

proc sort data=&data_set; by &independent_variables; run;
```

**6.2** How to check the encoded category reference? It easy, i did not delete the reference category. So, you can matching them by row.

**6.3** Test the normality of data.

```sas
*** make normality tests ***;
title color=brown "Normality Tests";

proc univariate data=&data_set normal;
    by &independent_variables;
    id &id;
    ods select TestsforNormality histogram qqplot;
    var &dependent_variables;
    histogram &dependent_variables/normal;
    qqplot &dependent_variables;
run;
```

**6.4** Use `cooksd`, `rstudentbyleverage` tests to detect outliers.

```sas
title color=bip "Outliers detection using (cooksd, rstudentbyleverage)";
ods graphics on;
*** i used defult cutoffs: rstudent cutoff=+-2 and cutoff Cook's is 4/(n-p) ***;

proc reg data=&data_set plots(only label)=(cooksd rstudentbyleverage);
    ODS SELECT CooksDPlot RStudentByLeverage;
    by &independent_variables;
    id &id;
    model &dependent_variables=&independent_variables;
run;
title;
```

**6.5** Also, use another test called `IQR` to detect outliers. (i'll mention it later)

**6.5.1** You need to transform data again. A column for needed parameters names (cmax, auc,...etc) and column for their values. 

```sas
*** transform data to make IQR and z-score tests***;
%let parameters= &dependent_variables;
%let length_parameters=%sysfunc(countw(&parameters));
data transformed_1;
    set &data_set;
    array parameters &parameters; * Create an array for the parameters;
    array parameters_names{&length_parameters} $ &parameters; * Corresponding names for the array;
    do i = 1 to &length_parameters; * Loop through each parameter;
        parameter_name = vname(parameters{i}); * Get the name of the variable;
        parameter_value = parameters{i}; * Get the value from the parameters array;
        output; * Output the transformed row;
    end;
    drop i &dependent_variables; * Drop the loop variable 'i' and transformed columns of parameters;
run;
proc sort data=transformed_1; by parameter_name &independent_variables; run; * needed step to avoid error in following steps;
```

**6.5.2** Here is the IQR test, This test is suitable for normal-distributed and skewed data too.

```sas
title color=bip "Outliers detection using (iqr)";
proc univariate data=transformed_1 noprint;
    by parameter_name &independent_variables;
    var parameter_value;
    output out=quartiles pctlpre=Q pctlpts=25, 75;
run;
data bounds_temp;
    set quartiles;
    IQR = Q75 - Q25; /* Calculate IQR */
    Lower_Bound_iqr = Q25 - 1.5 * IQR; /* Lower bound */
    Upper_Bound_iqr = Q75 + 1.5 * IQR; /* Upper bound */
run;
data iqr_results; * a merge step, here is the ooutliers detection;
    merge bounds_temp transformed_1; /* Get bounds */
    by parameter_name &independent_variables;
    if parameter_value < Lower_Bound_iqr or parameter_value > Upper_Bound_iqr then outlier = 'outlier';
    else outlier = '';
run;
proc print data=iqr_results; run;
```

**6.6** Also, use another test called `z-score` to detect outliers, this test highly sensitive to the mean, so if the data is not normally distributed, i prefer to ignore it. This test usually shows one outlier for each group.

```sas
title color=bibg "Outliers detection using (z-score)";
*** create new variable that shows z-scores for each raw data value***;
*** z-score cutoff=+-3;
proc sql;
create table result_Zscores as
    select transformed_1.*, (parameter_value - mean(parameter_value)) / std(parameter_value) as z_scores, case when (parameter_value - mean(parameter_value)) / std(parameter_value) NOT BETWEEN -3 AND 3 THEN "Outlier" ELSE "" END AS outlier_flag
    from transformed_1
    group by parameter_name, &sequ, &trt, &prd
    order by parameter_name, &sequ, &trt, &prd, &id;quit;
proc print data=result_Zscores; run;
```

**6.7** When you find more than three tests detected same outliers, or found a test that shows a very strong outliers, you can classify them as outliers.

**6.8** When you have outliers tell to the *PI*, and analyse your data **with** and **without** the outliers, and report your analysis with and without the outliers.

____

#### 7. Statistical Analysis

**7.1** I give you 2 ways to apply analysis tests, `PROC GLM` and `PROC MIXED`, you can compare each results to improve the code since I have not worked in real study yet.

**7.2** Prepare macros. [see 5.1 to calculate the logs of target parameters]

```sas
*make macro;
%let data_set=&data_set;
%let categ_variables= &SUBJ &sequ &PRD &TRT; * these are important, the code will not work without them;
%let log_paramerts_variables=log_cmax log_auc_0t; * needed parameters should be converted to log;
```

**7.3** Apply `PROC GLM`. And its `POWER`.

```sas
PROC GLM DATA=&data_set;
CLASS &categ_variables;
 MODEL &log_paramerts_variables = &sequ &SUBJ(&sequ) &PRD &TRT/ss3; * 'ss3' option tells SAS to use Type III sums of squares in the analysis, useful for balanced or unbalanced designs;
 RANDOM &SUBJ(&sequ) /TEST; *  &SUBJ(&sequ) is a random effect in the model, which accounts for variability between subjects in the analysis, 'TEST' option requests an F-test for the random effect;
 TEST H = &SEQU E = &SUBj(&SEQU); * The TEST statement is used to perform a hypothesis test, It tests the null hypothesis (H) about the effects of &SEQU and the random effects (E) of &SUBJ(&SEQU);
 LSMEANS &TRT / DIFF=CONTROL("R") CL ALPHA=0.1 ; * option specifies that differences should be calculated relative to the control level, CL requests confidence limits for the least squares means, and ALPHA=0.1 sets the confidence level at 90%;
 ODS OUTPUT LSMeanDiffCL=LSMD;
DATA LSMD; SET LSMD;
 GeoMRPointEstimate = EXP(DIFFERENCE); * calculates the geometric mean of the difference for each treatment contrast by taking the exponential of the DIFFERENCE (which was logged);
 GeoMRPointEstimate_LL = EXP(LowerCL); *  lower confidence limit for the geometric mean;
 GeoMRPointEstimate_UL = EXP(UpperCL); *  upper confidence limit for the geometric mean;
 cv_100_= 100*sqrt(exp(abs(Difference))-1); * This line calculates the coefficient of variation (expressed as a percentage) for the differences by taking the absolute value of the difference;
PROC PRINT DATA=LSMD; RUN;

* calc power;
proc sql noprint; * make macros for needed colculations in the proc power, in this macros i make lists to apply them as loop;
    select GeoMRPointEstimate into :GMRPE separated by ' ' from LSMD;
    select cv_100_ into :cv100 separated by ' ' from LSMD;
    select Dependent into :Dependent_var separated by ' ' from LSMD;
    select count(distinct &subj) into :ntotal from &my_data;quit;

%macro runPowerAnalysis(GMRPE, cv100, Dependent_var); * make a function that run a loop of power calculation.
    %let count = %sysfunc(countw(&GMRPE, %str(' '))); * Count the number of elements in GMRPE *you can put cv100 instead, becouse they have same length;
    * Loop through the elements of all lists;
    %do i = 1 %to &count;
        %let gmpe = %scan(&GMRPE, &i,%str(' ')); * Extract the ith element from GMRPE; 
        %let cv = %scan(&cv100, &i,%str(' ')); * Extract the ith element from cv100;
        %let Dependent = %scan(&Dependent_var, &i); * Extract the ith element from Dependent_var;

data _null_ ; * make the macros accepted to use into proc power;
    call symputx('GMRPE_', &gmpe);
    call symputx('cv100_', &cv/100);
    call symputx('n_total_', &ntotal); run;    

        TITLE "Power from GLM for &Dependent";
        proc power ; 
             twosamplemeans test=equiv_ratio 
             lower = 0.80
             upper = 1.25
             meanratio = &GMRPE_
             cv= &cv100_
             ntotal=&n_total_
             power = .
             alpha=.05; run; title;
%end; * End of loop ;
%mend runPowerAnalysis; * close the function (macro);
%runPowerAnalysis(&GMRPE, &cv100, &Dependent_var); * call the function;owerAnalysis(&GMRPE, &cv100, &Dependent_var);
```

**7.4** Apply `PROC MIXED`. And its `POWER`.

```sas
%macro run_mixed(data_set, categ_variables, paramerts_variables); * make a function that run a loop of MIXED calculation;
    %let num_vars = %sysfunc(countw(&paramerts_variables));
    %do i = 1 %to &num_vars;
        %let current_var = %scan(&paramerts_variables, &i);
        PROC MIXED DATA=&data_set;
          TITLE "--- Mixed for &current_var ---"; 
            CLASS &categ_variables;
            MODEL &current_var = &sequ &prd &TRT;
            random &subj(&sequ); * This accounts for variability the subject within sequence;
            LSMEANS &trt/PDIFF; * 'PDIFF' option requests pairwise differences between the least squares means, which can help identify differences in treatment effects;
            ESTIMATE 'T VS R' &TRT -1 1 /CL ALPHA=0.1; * It compares the treatment represented by T="1" against the control represented by R="-1" for the levels of &TRT, that becuase the R precedes the T in sorted code, CL option requests confidence limits for this estimate, and ALPHA=0.1 specifies that the confidence level should be set at 90%;
            ODS OUTPUT ESTIMATES=ESTIM CovParms=CovParms; RUN; * ESTIM for the estimates of the contrasts, CovParms for the covariance parameters associated with the random effects in the model;
        DATA ESTIM; 
            SET ESTIM;
            GeoMRPointEstimate= EXP(Estimate);
            GeoMRPointEstimate_LL = EXP(Lower);
            GeoMRPointEstimate_UL = EXP(Upper); 
            cv_100_= 100*sqrt(exp(abs(Estimate))-1);
            RUN;
        * calc power;
        proc sql noprint; * make macros for needed colculations in the proc power;
            select GeoMRPointEstimate into :GMRPE from ESTIM;
            select cv_100_ into :cv100 from ESTIM;
            select count(distinct &subj) into :ntotal from &my_data;quit;
        data _null_ ; * make the macros accepted to use into proc power;
            call symputx('GMRPE_', &GMRPE);
            call symputx('cv100_', &cv100/100);
            call symputx('n_total_', &ntotal); run;
        PROC PRINT DATA=ESTIM; 
            TITLE "Estimates for &current_var"; RUN; title;
            TITLE "Power from MIXED for &current_var";
        proc power ; 
             twosamplemeans test=equiv_ratio 
             lower = 0.80
             upper = 1.25
             meanratio = &GMRPE_
             cv= &cv100_
             ntotal=&n_total_
             power = .
             alpha=.05; run; title;
        %end; * end of loop;
%mend run_mixed; * close the function (macro);
%run_mixed(&data_set, &categ_variables, &log_paramerts_variables); *Call the function with your parameters;
```

____

#### 8. Report results

**8.1** Summary of Pharmacokinetic parameters of *Paracetamol* (ng/mL) for Test Product (T) table, and another for Reference Product (R) table:

```sas
*********************** Summary Table ************************;
proc sort data=&data_set; by &trt; run;

proc means data=&data_set n mean std min median max cv noprint;
by &trt;
vars cmax auc_0t t_max;
output out=Summary; run;
proc transpose data=summary (drop=_TYPE_ _freq_) name=varname out=summary;
by treatment;
id _STAT_; run;

ods graphics off;
proc surveymeans data=&data_set geomean cv ;
by &trt;
var cmax auc_0t t_max;
ods output statistics=cv_ geometricmeans=geomean_;
ods select cv_ geomean_; run;
ods graphics on;

proc sort data=summary; by &trt varname; run;
proc sort data=cv_; by &trt varname; run;
proc sort data=geomean_; by &trt varname; run;
data summary_2; merge summary cv_ geomean_; by &trt varname;

title 'Parameters Summary by ' &trt;
proc print data=summary_2; run; title;
```

**8.2** Make 2 comparative graphs between treatments. one for linear mean, second for mean log.

```sas
**************************** comparitive graphs ****************;
* Comparative mean linear graph;
proc sql; create table mean_values as select  &trt, time_point, avg(concentration_value) as concentration_value_means, avg(log(concentration_value)) as log_concentration_value_means
from transformed 
group by time_point, &trt; quit;
PROC sGPLOT DATA = mean_values; 
    series x=time_point y=concentration_value_means / group=&trt markers markerattrs=(symbol=circlefilled);
    xaxis label='Relative Time (hrs)'; yaxis label='concentration mean';
    title 'concentration mean vs Time (hrs)'; 
RUN; title;
* Comparative mean semi (log) graph;
PROC sGPLOT DATA = mean_values; 
    series x=time_point y=log_concentration_value_means / group=&trt markers markerattrs=(symbol=circlefilled);
    xaxis label='Relative Time (hrs)'; yaxis label='log concentration mean';
    title 'log concentration mean vs Time (hrs)'; 
RUN; title;
```

**8.3** Make 2 reports, one **without** outliers, second **with** outliers, and mention them in both reports, with used tests.

**8.4** Mention missing blood samples in a table: `Subject no.` , `Period`, `Time Point (hrs)`, but the `reason` will be filled by your *PI*.

*(To be continued)*

____

#### 9. Validations

*(I have no idea how to validate the results yet)*

____

#### 10. References

- Intra-Subject Coefficient of Variation (CV%) for Sample Size Estimation for Crossover Design: https://onbiostatistics.blogspot.com/2014/03/intra-subject-coefficient-of-variation.html

- Bioequivalence data analysis [tcp.2020.28.e20](https://www.tcpharm.org/pdf/10.12793/tcp.2020.28.e20)

- [On Biostatistics and Clinical Trials: Cookbook SAS Codes for Bioequivalence Test in 2x2x2 Crossover Design](https://onbiostatistics.blogspot.com/2012/04/cookbook-sas-codes-for-bioequivalence.html)

- 

*(To be continued)*
